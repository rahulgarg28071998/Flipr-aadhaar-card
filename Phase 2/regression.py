import matplotlib 
matplotlib.use('Agg')
from keras.callbacks import ModelCheckpoint, EarlyStopping
import LossHistory as LH
from sklearn import cross_validation
from keras.preprocessing.image import ImageDataGenerator
from keras.models import Sequential
from keras.layers import Dense, Flatten
from keras.layers import Convolution2D, MaxPooling2D, Lambda
from keras.optimizers import SGD
import pool
#from data import data_utils
from data_aquisition_group import group_extraction
from matplotlib import pyplot as plt
import numpy as np
import h5py

batch_size = 1024
nb_epoch = 50
data_augmentation = True

X_train, X_test, y_train, y_test = group_extraction()
X_train, X_val, y_train, y_val = cross_validation.train_test_split(X_train,
                                                                      y_train, test_size=0.0, random_state=42)
model = Sequential()
model.add(Convolution2D(40, 5, 5, input_shape=(1, 48, 48)))
model.add(MaxPooling2D(pool_size=(4, 4)))
model.add(Convolution2D(80, 5, 5))
model.add(Lambda(pool.min_max_pool2d, output_shape=pool.min_max_pool2d_output_shape))
model.add(Flatten())
model.add(Dense(1024))
model.add(Dense(1024))
model.add(Dense(1))


model.compile(loss='mse', #mean absolute error
              optimizer = 'adam')
print (model.summary())
#model.load_weights('weights.hdf5')
checkpointer = ModelCheckpoint(filepath="weights_cvit_data.hdf5", verbose=1, save_best_only=True)
early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='auto')

history = LH.LossHistory() # Object of the class history
print(X_train.shape)
if not data_augmentation:
    print('Not using data augmentation.')
    model.fit(X_train, y_train,
              batch_size=batch_size,
              nb_epoch=1,
              validation_data=(X_test, y_test),
              shuffle=True)
else:
    print('Using real-time data augmentation.')

    # this will do pre-processing and realtime data augmentation
    datagen = ImageDataGenerator(
        width_shift_range=0.10,  # randomly shift images horizontally (fraction of total width)
        height_shift_range=0.10,  # randomly shift images vertically (fraction of total height)
        vertical_flip=False,
        zoom_range=0.1,
        shear_range=0.1,
        fill_mode="nearest",
        zca_whitening = False)  # randomly flip images

    # compute quantities required for featurewise normalization
    # (std, mean, and principal components if ZCA whitening is applied)
    datagen.fit(X_train)

    # fit the model on the batches generated by datagen.flow()
    model.fit_generator(datagen.flow(X_train, y_train,
                                     batch_size=batch_size),
                        samples_per_epoch=X_train.shape[0],
                        nb_epoch=nb_epoch,
                        callbacks=[early_stopping, history, checkpointer],
                        validation_data = (X_test, y_test))

score = model.predict(X_test, batch_size=32)

# Extracting the losses and accuracies
val_losses = history.val_losses_epoch
val_axis = np.arange(len(val_losses))
losses_batch = history.losses_batch
loss_batch_axis = np.arange(len(losses_batch))

with h5py.File('results.hdf5', 'w') as hf:
    hf.create_dataset('val_losses', val_losses)
    hf.create_dataset('losses_batch', losses_batch)
    #hf.create_dataset('score', score)
    #hf.createdataset('val_losses', val_losses)  

plt.subplot(1, 2, 1)
plt.plot(val_axis, val_losses, color="blue", label="validation_loss")
plt.subplot(1, 2, 2)
plt.plot(loss_batch_axis, losses_batch, color="red", label="training_loss")
plt.savefig('losses.png')
